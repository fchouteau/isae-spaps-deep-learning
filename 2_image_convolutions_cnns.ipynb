{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "984c2080",
   "metadata": {
    "id": "ab8a63a8"
   },
   "source": [
    "# Deep Learning for Computer Vision\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| Quentin LÃ©turgie | Florient Chouteau | <a href=\"https://supaerodatascience.github.io/deep-learning/\">https://supaerodatascience.github.io/deep-learning/</a>\n",
    "\n",
    "## Session 2 : Images, Convolutions and CNN\n",
    "\n",
    "Welcome to this BE about applying images, convolutions and CNN. This is the second notebook of Deep Learning for Computer Vision\n",
    "\n",
    "1.   Generality about deep learning\n",
    "2.   **Images, Convolutions and CNN**\n",
    "3.   CNN classifier\n",
    "\n",
    "It is recommended to use **Google Colab** to run these notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f921fe",
   "metadata": {
    "id": "ANrvYsU8_HjY"
   },
   "source": [
    "## **Session 2.1** : About images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa124644",
   "metadata": {
    "id": "946785cc"
   },
   "source": [
    "A digital image is an image composed of picture elements, also known as pixels, each with finite, discrete quantities of numeric representation for its intensity or gray level that is an output from its two-dimensional functions fed as input by its spatial coordinates denoted with x, y on the x-axis and y-axis, respectively.\n",
    "\n",
    "We represent images as matrixes,\n",
    "\n",
    "Images are made of pixels, and pixels are made of combinations of primary colors (in our case Red, Green and Blue). In this context, images have chanels that are the grayscale image of the same size as a color image, made of just one of these primary colors. For instance, an image from a standard digital camera will have a red, green and blue channel. A grayscale image has just one channel.\n",
    "\n",
    "In remote sensing, channels are often referred to as raster bands.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*icINeO4H7UKe3NlU1fXqlA.jpeg\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "For the rest of this workshop we will use the following axis conventions for images\n",
    "\n",
    "![conventions](https://storage.googleapis.com/fchouteau-isae-deep-learning/static/image_coordinates.png)\n",
    "\n",
    "The reference library in python for working with images is https://scikit-image.org/\n",
    "\n",
    "We will just do basic image manipulation, but you [can look at all the examples](https://scikit-image.org/docs/stable/auto_examples/) if you need to get a better grasp of image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29c7ed0",
   "metadata": {
    "id": "0e379e97"
   },
   "outputs": [],
   "source": [
    "### LET'S CODE ###\n",
    "\n",
    "# import some usefull library\n",
    "import numpy as np\n",
    "import skimage\n",
    "import skimage.data\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91ce8b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "id": "4ada1b9e",
    "outputId": "8dc98938-ff32-42d8-ff33-5c35a428a9dc"
   },
   "outputs": [],
   "source": [
    "# Load an image from skimage library\n",
    "my_image = skimage.data.astronaut()\n",
    "\n",
    "# Display the astronaut image\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(my_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6656278f",
   "metadata": {
    "id": "b870a67f"
   },
   "source": [
    "* What is the height, width and number of channels of this image ?\n",
    "* In which order is the data represented ? Which dimensions are channels in ?\n",
    "* What is the image \"dtype\" ?\n",
    "\n",
    "How to get those informations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db5d849",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ec3ccde",
    "outputId": "4a356fc5-80d7-4d31-a015-19b2336442f8"
   },
   "outputs": [],
   "source": [
    "# Get image shape\n",
    "img_height, img_width, img_channel = my_image.shape\n",
    "\n",
    "print(\"height = \", img_height, \"pixels\")\n",
    "print(\"width = \", img_width, \"pixels\")\n",
    "print(\"Number of channels : \", img_channel)\n",
    "print(\"Image data type : \", my_image.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ee9600",
   "metadata": {
    "id": "878710a2"
   },
   "source": [
    "**uint8** means **unsigned interger on 8 bits**. This means that our image pixel values are integer between **0** and **255**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429ff7f8",
   "metadata": {
    "id": "d4e251ea"
   },
   "source": [
    "### Exercice 1 : Extract subpart of the image\n",
    "In the following code, replace `...` by your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9805ef0c",
   "metadata": {
    "id": "550fdec3"
   },
   "outputs": [],
   "source": [
    "# Extract subpart of the image (similar to crop), center on astronaut face.\n",
    "# Define pixel position where astronaut face starts and ends for .\n",
    "i_start = ...  # Pixel position between 0-512 and lower than i_end\n",
    "i_end = ...  # Pixel position between 0-512 and higher than i_start\n",
    "j_start = ...  # Pixel position between 0-512 and lower than i_end\n",
    "j_end = ...  # Pixel position between 0-512 and higher than j_start\n",
    "\n",
    "\n",
    "# Extract subpart of the image and display it\n",
    "sub_img = my_image[i_start:i_end, j_start:j_end]\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(sub_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929aaa7d",
   "metadata": {
    "id": "51bf8232"
   },
   "source": [
    "### *Solution : Exercice 1* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87e74d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "ca11237b",
    "outputId": "054d8637-975e-425c-80fa-c3a71e67af46"
   },
   "outputs": [],
   "source": [
    "# Extract subpart of the image (similar to crop), center on astronaut face.\n",
    "# Define pixel position where astronaut face starts and ends.\n",
    "i_start = 0  # Pixel position between 0-512 and lower than i_end\n",
    "i_end = 200  # Pixel position between 0-512 and higher than i_start\n",
    "j_start = 150  # Pixel position between 0-512 and lower than i_end\n",
    "j_end = 300  # Pixel position between 0-512 and higher than j_start\n",
    "\n",
    "\n",
    "# Extract subpart of the image and display it\n",
    "sub_img = my_image[i_start:i_end, j_start:j_end]\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(sub_img)\n",
    "plt.show()\n",
    "\n",
    "# We can get subpart image shape information\n",
    "sub_img_height, sub_img_width, sub_img_channel = sub_img.shape\n",
    "\n",
    "print(\"height = \", sub_img_height, \"pixels\")\n",
    "print(\"width = \", sub_img_width, \"pixels\")\n",
    "print(\"Number of channels : \", sub_img_channel)\n",
    "print(\"Image data type : \", my_image.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5e822e",
   "metadata": {
    "id": "c36fff05"
   },
   "source": [
    "In classical image representation, we use the [RGB color model](https://en.wikipedia.org/wiki/RGB_color_model) where the image is represented by three R,G,B channels (in that order).\n",
    "\n",
    "Usually we also use 8bits color depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f0319b",
   "metadata": {
    "id": "f9e3f807"
   },
   "source": [
    "### Exercice 2 : Play with channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbd3d04",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "id": "414550d8",
    "outputId": "311f2f76-aaef-42f9-d610-66474a5d4f59"
   },
   "outputs": [],
   "source": [
    "# Delete one color ot the image. For instance, set the red channel pixels value to 0.\n",
    "# Display the image and check colors (especially red)\n",
    "channel_number_to_delete = ...\n",
    "\n",
    "img_without_red = np.copy(my_image)\n",
    "img_without_red[:, :, channel_number_to_delete] = 0\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(img_without_red)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d2164d",
   "metadata": {
    "id": "cd7d87e3"
   },
   "outputs": [],
   "source": [
    "# Do the same for green band\n",
    "channel_number_to_delete = ...\n",
    "\n",
    "img_without_green = np.copy(my_image)\n",
    "img_without_green[:, :, channel_number_to_delete] = 0\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(img_without_green)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b4916c",
   "metadata": {
    "id": "2ec71b08"
   },
   "outputs": [],
   "source": [
    "# Do the same for blue band\n",
    "channel_number_to_delete = ...\n",
    "\n",
    "img_without_blue = np.copy(my_image)\n",
    "img_without_blue[:, :, channel_number_to_delete] = 0\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(img_without_blue)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5d3afa",
   "metadata": {
    "id": "0ca599fc"
   },
   "source": [
    "### *Solution : Exercice 2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72350be0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 935
    },
    "id": "eeaf24cd",
    "outputId": "47881f36-5977-4bef-b11a-0926b885320d"
   },
   "outputs": [],
   "source": [
    "# Delete red band\n",
    "channel_number_to_delete = 0\n",
    "\n",
    "img_without_red = np.copy(my_image)\n",
    "img_without_red[:, :, channel_number_to_delete] = 0\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(img_without_red)\n",
    "plt.show()\n",
    "\n",
    "# Delete green band\n",
    "channel_number_to_delete = 1\n",
    "\n",
    "img_without_green = np.copy(my_image)\n",
    "img_without_green[:, :, channel_number_to_delete] = 0\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(img_without_green)\n",
    "plt.show()\n",
    "\n",
    "# Delete green band\n",
    "channel_number_to_delete = 2\n",
    "\n",
    "img_without_blue = np.copy(my_image)\n",
    "img_without_blue[:, :, channel_number_to_delete] = 0\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(img_without_blue)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2cc89d",
   "metadata": {
    "id": "8e93aea7"
   },
   "source": [
    "## **Session 2.2 :** Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cd1bbb",
   "metadata": {
    "id": "81a0309c"
   },
   "source": [
    "Someone may have told you that CNNs were the \"thing\" that made deep learning for image processing possible. But what are convolutions ?\n",
    "\n",
    "First, remember that you [learnt about convolutions a long time ago ð±](https://fr.wikipedia.org/wiki/Produit_de_convolution)\n",
    "\n",
    "<img src=\"https://betterexplained.com/ColorizedMath/img/Convolution.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "So basically, we slide a filter over the signal. In 2D, this means\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/535/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "One thing you can notice is that if we slide a filter over an image we \"lose\" pixels at the border. This is actually quite easy to compute : assuming a of size `2*k +1` we loose `k` pixels on each side of the image in each direction.\n",
    "\n",
    "![](https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/no_padding_no_strides.gif)\n",
    "\n",
    "If you want to get them back you have to \"pad\" (add values at the border, for examples zeroes) the image\n",
    "\n",
    "![](https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/arbitrary_padding_no_strides.gif)\n",
    "\n",
    "For more information, this website is excellent : https://cs231n.github.io/convolutional-networks/#conv\n",
    "\n",
    "Let's play with convolutions a little bit before actually doing CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec76029",
   "metadata": {
    "id": "8aadfdde"
   },
   "source": [
    "### 2D Convolution without \"depth\"\n",
    "\n",
    "First, let's look at basic filtering over grayscale (1 channel) images. We will slide a filter over H,W spatial dimensions and get the result\n",
    "\n",
    "First, the convolution implementation without depth function is : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eb1e23",
   "metadata": {
    "id": "9545d023"
   },
   "outputs": [],
   "source": [
    "def convolve(img: np.array, kernel: np.array) -> np.array:\n",
    "    \"\"\"Apply a convolutionel kernel k on image img and return convolved image\"\"\"\n",
    "    k = kernel.shape[0]\n",
    "    h, w = img.shape[:2]\n",
    "    p = int(k // 2)\n",
    "\n",
    "    # Build output array : 2D array of zeros\n",
    "    kernel = kernel.astype(np.float32)\n",
    "    img = img.astype(np.float32)\n",
    "    convolved_img = np.zeros(shape=(h - 2 * p, w - 2 * p)).astype(np.float32)\n",
    "\n",
    "    # Iterate over the rows\n",
    "    for i in range(h - 2 * p):\n",
    "        # Iterate over the columns\n",
    "        for j in range(w - 2 * p):\n",
    "            # img[i, j] = individual pixel value\n",
    "            # Get the current matrix\n",
    "            mat = img[i : i + k, j : j + k]\n",
    "\n",
    "            # Apply the convolution - element-wise multiplication and summation of the result\n",
    "            # Store the result to i-th row and j-th column of our convolved_img array\n",
    "            convolved_img[i, j] = np.sum(np.multiply(mat, kernel))\n",
    "\n",
    "    convolved_img = convolved_img.clip(0.0, 255.0).astype(np.uint8)\n",
    "\n",
    "    return convolved_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bc8f31",
   "metadata": {
    "id": "85fec939"
   },
   "source": [
    "Let's try some filters.\n",
    "\n",
    "### Filter 1\n",
    "What happens if I use this filter as input ?\n",
    "\n",
    "![identity](https://wikimedia.org/api/rest_v1/media/math/render/svg/1fbc763a0af339e3a3ff20af60a8a993c53086a7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e62e7b0",
   "metadata": {
    "id": "e46ba213"
   },
   "outputs": [],
   "source": [
    "# Build convolution kernel\n",
    "k = [[0, 0, 0], [0, 1, 0], [0, 0, 0]]\n",
    "k = np.asarray(k)\n",
    "\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f6c587",
   "metadata": {
    "id": "ce967c10"
   },
   "outputs": [],
   "source": [
    "# Load image with only first channel\n",
    "img = skimage.data.astronaut()\n",
    "img = img[:, :, 0]\n",
    "\n",
    "# Apply convolution kernel on image\n",
    "convolved_img = convolve(img, k)\n",
    "\n",
    "# Compare shapes\n",
    "print(\"Shape before convolution (height, width) : \", img.shape)\n",
    "print(\"Shape after convolution (height, width) : \", convolved_img.shape)\n",
    "\n",
    "# Compare images\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].set_title(\"Before convolution\")\n",
    "ax[0].imshow(img, cmap=\"gray\")\n",
    "ax[1].set_title(\"After convolution\")\n",
    "ax[1].imshow(convolved_img, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d425791a",
   "metadata": {
    "id": "b74dffd1"
   },
   "source": [
    "What does this filter do? Nothing... It is identity filter.\n",
    "\n",
    "Note the loss of 2 pixels height and 2 pixels width... If we wanted to alleviate it we could do something like padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb87b633",
   "metadata": {
    "id": "82aea72b"
   },
   "source": [
    "### Filter 2\n",
    "Too easy ! Let's try another filter\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/91256bfeece3344f8602e288d445e6422c8b8a1c)\n",
    "\n",
    "What does it do ? Take a guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0dd8de",
   "metadata": {
    "id": "d22ef50d"
   },
   "outputs": [],
   "source": [
    "k = np.asarray([[1, 1, 1], [1, 1, 1], [1, 1, 1]]).astype(np.float32)\n",
    "k = k / k.sum()\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e708d5",
   "metadata": {
    "id": "811f0e73"
   },
   "outputs": [],
   "source": [
    "# Load image with only first channel\n",
    "img = skimage.data.astronaut()\n",
    "img = img[:, :, 0]\n",
    "\n",
    "# Apply convolution kernel on image\n",
    "convolved_img = convolve(img, k)\n",
    "\n",
    "# Compare shapes\n",
    "print(\"Shape before convolution (height, width) : \", img.shape)\n",
    "print(\"Shape after convolution (height, width) : \", convolved_img.shape)\n",
    "\n",
    "# Compare images\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].set_title(\"Before convolution\")\n",
    "ax[0].imshow(img, cmap=\"gray\")\n",
    "ax[1].set_title(\"After convolution\")\n",
    "ax[1].imshow(convolved_img, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665e36a5",
   "metadata": {
    "id": "10ad88be"
   },
   "source": [
    "What does this filter do? Blurring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e353b05e",
   "metadata": {
    "id": "1ff7ca22"
   },
   "source": [
    "### Filter 3\n",
    "Two more filters for edge detection (Sobel filter)\n",
    "\n",
    "![image-2.png](attachment:image-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cef9e0",
   "metadata": {
    "id": "9e78351a"
   },
   "outputs": [],
   "source": [
    "k_h = [[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]\n",
    "\n",
    "k_v = [[-1, -2, -1], [0, 0, 0], [1, 2, 1]]\n",
    "\n",
    "k_h = np.asarray(k_h)\n",
    "k_v = np.asarray(k_v)\n",
    "\n",
    "print(\"k_h :\")\n",
    "print(k_h)\n",
    "print()\n",
    "print(\"k_v :\")\n",
    "print(k_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ff608f",
   "metadata": {
    "id": "a7fd79e2"
   },
   "outputs": [],
   "source": [
    "# Load image with only first channel\n",
    "img = skimage.data.astronaut()\n",
    "img = img[:, :, 0]\n",
    "\n",
    "# Apply convolution kernel on image\n",
    "convolved_h_img = convolve(img, k_h)\n",
    "convolved_v_img = convolve(img, k_v)\n",
    "\n",
    "# Compare shapes\n",
    "print(\"Shape before convolution (height, width) : \", img.shape)\n",
    "print(\"Shape after convolution k_h (height, width) : \", convolved_h_img.shape)\n",
    "print(\"Shape after convolution k_v (height, width) : \", convolved_v_img.shape)\n",
    "\n",
    "# Compare images\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "ax[0].set_title(\"Before convolution\")\n",
    "ax[0].imshow(img, cmap=\"gray\")\n",
    "ax[1].set_title(\"After convolution by k_h\")\n",
    "ax[1].imshow(convolved_h_img, cmap=\"gray\")\n",
    "ax[2].set_title(\"After convolution by v_h\")\n",
    "ax[2].imshow(convolved_v_img, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Sum those 2 results : \")\n",
    "plt.figure()\n",
    "plt.title(\"Sum of convolution by k_h and convolution by k_v\")\n",
    "plt.imshow(convolved_h_img + convolved_v_img, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda38388",
   "metadata": {
    "id": "4d7d8779"
   },
   "source": [
    "What does this filter do? Edge detection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72644f0f",
   "metadata": {
    "id": "618de390"
   },
   "source": [
    "If we wanted, we could learn the filters in order to do... cat classification, plane classification !\n",
    "\n",
    "There are many more filters that have been designed to do interesting things, you can find an interesting list here : https://en.wikipedia.org/wiki/Kernel_(image_processing)\n",
    "\n",
    "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F2141203%2F99dba888571cd6284b9b59903061aaa4%2Fko001.png?generation=1591783791920610&alt=media)\n",
    "\n",
    "**Takeaway message** : Kernel filtering (convolution) takes its root from classical image processing !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb69a181",
   "metadata": {
    "id": "36ff73a6"
   },
   "source": [
    "### Convolutions with depth\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/FjvuN.gif\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "This is a convolution operator. It's the same as above, except our filter takes all channels of the image as input. So basically a \"Convolution\" layer is a filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8879e2",
   "metadata": {
    "id": "95568999"
   },
   "source": [
    "**Important** : In classical image processing, we use the (height, width, channels) convention, however in torch we prefer using (channels, height, width) convention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95422967",
   "metadata": {
    "id": "ff031e9a"
   },
   "outputs": [],
   "source": [
    "# Load image with 3 channels\n",
    "img = skimage.data.astronaut()\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d855a5ba",
   "metadata": {
    "id": "292f18b6"
   },
   "outputs": [],
   "source": [
    "# To transpose an image, we use\n",
    "img = img.transpose((2, 0, 1))  # change channel order\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab32c6a",
   "metadata": {
    "id": "4a1717fa"
   },
   "outputs": [],
   "source": [
    "# This is the general implementation of convolutions\n",
    "# It is the same function as function 'convolve' defined above but for multiple channel\n",
    "def forward_convolution(conv_W, conv_b, data):\n",
    "    \"\"\"\n",
    "    Compute the output from a convolutional layer given the weights and data.\n",
    "\n",
    "    conv_W is of the shape (# output channels, # input channels, convolution width, convolution height )\n",
    "    output_channels is the number of filters in the convolution\n",
    "\n",
    "    conv_b is of the shape (# output channels)\n",
    "\n",
    "    data is of the shape (# input channels, width, height)\n",
    "\n",
    "    The output should be the result of a convolution and should be of the size:\n",
    "        (# output channels, width - convolution width + 1, height -  convolution height + 1)\n",
    "\n",
    "    Returns:\n",
    "        The output of the convolution as a numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    conv_channels, _, conv_width, conv_height = conv_W.shape\n",
    "\n",
    "    input_channels, input_width, input_height = data.shape\n",
    "\n",
    "    output = np.zeros(\n",
    "        (conv_channels, input_width - conv_width + 1, input_height - conv_height + 1)\n",
    "    )\n",
    "\n",
    "    for x in range(input_width - conv_width + 1):\n",
    "        for y in range(input_height - conv_height + 1):\n",
    "            for output_channel in range(conv_channels):\n",
    "                output[output_channel, x, y] = (\n",
    "                    np.sum(\n",
    "                        np.multiply(\n",
    "                            data[:, x : (x + conv_width), y : (y + conv_height)],\n",
    "                            conv_W[output_channel, :, :, :],\n",
    "                        )\n",
    "                    )\n",
    "                    + conv_b[output_channel]\n",
    "                )\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02283232",
   "metadata": {
    "id": "b0b85d17",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We define random convolution kernel (weights) and random biases\n",
    "weights = np.random.random((1, 3, 3, 3))\n",
    "biases = np.random.random((3,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3412b648",
   "metadata": {
    "id": "82c32662",
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convolve the input with the weights and bias\n",
    "convolved_img = forward_convolution(weights, biases, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a157eb6",
   "metadata": {
    "id": "4027bb79"
   },
   "outputs": [],
   "source": [
    "# Compare shapes\n",
    "print(\"Shape before convolution (number of channels, height, width) : \", img.shape)\n",
    "print(\n",
    "    \"Shape after convolution (number of channel, height, width) : \", convolved_img.shape\n",
    ")\n",
    "\n",
    "# Compare images (Don't forget that matplotlib uses (h,w,c) to plot images !)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].set_title(\"Before convolution\")\n",
    "ax[0].imshow(img.transpose((1, 2, 0)))\n",
    "ax[1].set_title(\"After convolution\")\n",
    "ax[1].imshow(convolved_img[0], cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "# Print filter and biases values\n",
    "print(f\"Filter:\\n {weights}\")\n",
    "print()\n",
    "print(\"Bias:\", biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b544788c",
   "metadata": {
    "id": "27e59773"
   },
   "source": [
    "What does this filter do? An unknown random operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae92429e",
   "metadata": {
    "id": "54c1ded4"
   },
   "source": [
    "Some useful resources for more information :\n",
    "\n",
    "- The DL class https://github.com/fchouteau/deep-learning/blob/main/deep/Deep%20Learning.ipynb\n",
    "- https://github.com/vdumoulin/conv_arithmetic\n",
    "- https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b3f2de",
   "metadata": {
    "id": "b79ec18d"
   },
   "source": [
    "## **Session 2.3** : Convolution Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482f39b6",
   "metadata": {
    "id": "d39b3fa5"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "# Ce texte est au format code\n",
    "```\n",
    "\n",
    "I shamelessly copy pasted code from this excellent class : https://github.com/Atcold/pytorch-Deep-Learning/blob/master/06-convnet.ipynb\n",
    "\n",
    "Remember, an Artificial Neural Network is a stack of \n",
    "\n",
    "- \"Fully Connected\" layers\n",
    "- Non linearities\n",
    "\n",
    "A Convolutional Neural Network is a stack of\n",
    "- Convolutional Layers aka Filter Banks\n",
    "    - Increase dimensionality\n",
    "    - Projection on overcomplete basis\n",
    "    - Edge detections\n",
    "- Non-linearities\n",
    "    - Sparsification\n",
    "    - Typically Rectified Linear Unit (ReLU): ReLU(x)=maxâ¡(x,0)\\text{ReLU}(x) = \\max(x, 0)ReLU(x)=max(x,0)\n",
    "- Pooling\n",
    "    - Aggregating over a feature map\n",
    "    - Example : Maximum\n",
    "\n",
    "![](https://cdn-media-1.freecodecamp.org/images/Dgy6hBvOvAWofkrDM8BclOU3E3C2hqb25qBb)\n",
    "\n",
    "<img src=\"https://production-media.paperswithcode.com/methods/MaxpoolSample2.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "Max pooling operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c8e6cd",
   "metadata": {
    "id": "a7db0840"
   },
   "source": [
    "Why do CNNs works ?\n",
    "\n",
    "To perform well, we need to incorporate some prior knowledge about the problem\n",
    "\n",
    "    Assumptions helps us when they are true\n",
    "    They hurt us when they are not\n",
    "    We want to make just the right amount of assumptions, not more than that\n",
    "    \n",
    "In Deep Learning\n",
    "\n",
    "    Many layers: compositionality\n",
    "    Convolutions: locality + stationarity of images\n",
    "    Pooling: Invariance of object class to translations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e795cb",
   "metadata": {
    "id": "iuqVb4bS2lTP"
   },
   "source": [
    "In pytorch many function exists to define layers easily and are stored in `torch.nn` package :\n",
    "\n",
    "\n",
    "*   `nn.Conv2d` : Build 2 dimension convolutional layer\n",
    "*   `nn.ReLU` : Build Rectified Linear Unit layer\n",
    "*   `nn.MaxPool2d` : Build 2 dimension max pooling layer\n",
    "\n",
    "\n",
    "To see all available layers : https://pytorch.org/docs/stable/nn.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a554468",
   "metadata": {
    "id": "KMV96Yh42RIG"
   },
   "outputs": [],
   "source": [
    "### LET'S CODE ###\n",
    "# We will build our first cnn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66234f57",
   "metadata": {
    "id": "gofB8nTF9AVU"
   },
   "source": [
    "CNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcfb801",
   "metadata": {
    "id": "leXC3D1R2hcY"
   },
   "outputs": [],
   "source": [
    "# We define a CNN through a python class\n",
    "class CNN_Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Network, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=12, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(24 * 4 * 4, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = nn.MaxPool2d(kernel_size=2)(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = nn.MaxPool2d(kernel_size=2)(x)\n",
    "\n",
    "        x = nn.Flatten()(x)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = nn.LogSoftmax(dim=1)(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "cnn_model = CNN_Network()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7fe6b1",
   "metadata": {
    "id": "is_k8NsZ8YzC"
   },
   "source": [
    "DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4157d103",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "YKBLuDRV5SXj",
    "outputId": "1b049df0-b8d1-4044-b44e-ce8a63f258a5"
   },
   "outputs": [],
   "source": [
    "# To build our model we need to define input and output shapes\n",
    "# For this example we use basic dataset : mnist. We load it thanks to PyTorch\n",
    "mnist_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),  # Transform image to tensor\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ]\n",
    ")  # Normalize each sample by global mean/std\n",
    "\n",
    "mnist_train_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=mnist_transform\n",
    ")\n",
    "mnist_test_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=mnist_transform\n",
    ")\n",
    "\n",
    "\n",
    "# Print some stats\n",
    "print(\"Number of samples in MNIST train dataset : \", len(mnist_train_dataset))\n",
    "print(\"Number of samples in MNIST test dataset : \", len(mnist_test_dataset))\n",
    "print(\"Dataset image shape : \", np.asarray(mnist_train_dataset[0][0]).shape)\n",
    "print()\n",
    "\n",
    "# To visualize this train dataset, we plot few samples\n",
    "n = 10\n",
    "print(n, \"samples of MNIST train dataset\")\n",
    "\n",
    "print(\"train labels : \", end=\"\")\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i in range(n):\n",
    "    print(mnist_train_dataset[i][1], end=\", \")\n",
    "    plt.subplot(1, n, i + 1)\n",
    "    plt.imshow(mnist_train_dataset[i][0][0], cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print()\n",
    "# To visualize this test dataset, we plot few samples\n",
    "n = 10\n",
    "print(n, \"samples of MNIST test dataset\")\n",
    "\n",
    "print(\"test labels : \", end=\"\")\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i in range(n):\n",
    "    print(mnist_test_dataset[i][1], end=\", \")\n",
    "    plt.subplot(1, n, i + 1)\n",
    "    plt.imshow(mnist_test_dataset[i][0][0], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea7583e",
   "metadata": {
    "id": "NBLmPXwVPkPa"
   },
   "source": [
    "DATA LOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09926b13",
   "metadata": {
    "id": "3WuIfjiZPmDw"
   },
   "outputs": [],
   "source": [
    "input_size = 28 * 28  # images are 28x28 pixels\n",
    "output_size = 10  # there are 10 classes\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    mnist_train_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    mnist_test_dataset, batch_size=1000, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df1d26a",
   "metadata": {
    "id": "10762497"
   },
   "source": [
    "Switching between CPU and GPU in PyTorch is controlled via a device string, which will seemlessly determine whether GPU is available, falling back to CPU if not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0126feda",
   "metadata": {
    "id": "2a09379c"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d1e98b",
   "metadata": {
    "id": "P3brB4wrUA41"
   },
   "source": [
    "TRAINING AND TESTING STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c92b28f",
   "metadata": {
    "id": "A2xQGm1XVOV4"
   },
   "outputs": [],
   "source": [
    "accuracy_list = []\n",
    "\n",
    "\n",
    "def train(epoch, model):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # send data and model to same device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Zero grad optimize\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Apply the model on data and get output prediction\n",
    "        prediction = model(data)\n",
    "\n",
    "        # Compute loss between prediction and target\n",
    "        loss = F.nll_loss(prediction, target)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "def test(model, perm=torch.arange(0, 784).long()):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        # send data and model to same device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Apply the model on data and get output prediction\n",
    "        prediction = model(data)\n",
    "        test_loss += F.nll_loss(\n",
    "            prediction, target, reduction=\"sum\"\n",
    "        ).item()  # sum up batch loss\n",
    "        pred = prediction.data.max(1, keepdim=True)[\n",
    "            1\n",
    "        ]  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100.0 * correct / len(test_loader.dataset)\n",
    "    accuracy_list.append(accuracy)\n",
    "    print(\n",
    "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss, correct, len(test_loader.dataset), accuracy\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d991c79",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "UcXXkBnDW10I",
    "outputId": "3d04ec9b-3a03-4410-dea3-a39d00393cac"
   },
   "outputs": [],
   "source": [
    "model = CNN_Network()\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    train(epoch, model)\n",
    "    test(cnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58660780",
   "metadata": {
    "id": "715b8f46"
   },
   "outputs": [],
   "source": [
    "accuracy_list = []\n",
    "\n",
    "\n",
    "def train(epoch, model, perm=torch.arange(0, 784).long()):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # send to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # permute pixels\n",
    "        data = data.view(-1, 28 * 28)\n",
    "        data = data[:, perm]\n",
    "        data = data.view(-1, 1, 28, 28)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "def test(model, perm=torch.arange(0, 784).long()):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        # send to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # permute pixels\n",
    "        data = data.view(-1, 28 * 28)\n",
    "        data = data[:, perm]\n",
    "        data = data.view(-1, 1, 28, 28)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(\n",
    "            output, target, reduction=\"sum\"\n",
    "        ).item()  # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[\n",
    "            1\n",
    "        ]  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100.0 * correct / len(test_loader.dataset)\n",
    "    accuracy_list.append(accuracy)\n",
    "    print(\n",
    "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss, correct, len(test_loader.dataset), accuracy\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084a21f3",
   "metadata": {
    "id": "a723ed09"
   },
   "outputs": [],
   "source": [
    "# function to count number of parameters\n",
    "def get_n_params(model):\n",
    "    np = 0\n",
    "    for p in list(model.parameters()):\n",
    "        np += p.nelement()\n",
    "    return np\n",
    "\n",
    "\n",
    "# Create two models: One ANN vs One CNN\n",
    "class FullyConnected2Layers(nn.Module):\n",
    "    def __init__(self, input_size, n_hidden, output_size):\n",
    "        super(FullyConnected2Layers, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, output_size),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, n_feature, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.n_feature = n_feature\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=n_feature, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(n_feature, n_feature, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(n_feature * 4 * 4, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 10),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, verbose=False):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a1b279",
   "metadata": {
    "id": "70c2a4cf"
   },
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a4f238",
   "metadata": {
    "id": "4cfbe195"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5393829b",
   "metadata": {
    "id": "2dff475e"
   },
   "outputs": [],
   "source": [
    "input_size = 28 * 28  # images are 28x28 pixels\n",
    "output_size = 10  # there are 10 classes\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        \"../data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        \"../data\",\n",
    "        train=False,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=1000,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41e5553",
   "metadata": {
    "id": "43644c21"
   },
   "outputs": [],
   "source": [
    "# show some images\n",
    "plt.figure(figsize=(16, 6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    image, _ = train_loader.dataset.__getitem__(i)\n",
    "    plt.imshow(image.squeeze().numpy())\n",
    "    plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4ee07e",
   "metadata": {
    "id": "169507f0"
   },
   "source": [
    "### CNNs vs Fully Connected Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170d8c98",
   "metadata": {
    "id": "f839a688"
   },
   "source": [
    "A small FullyConnected ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5465110a",
   "metadata": {
    "id": "6e6dc883"
   },
   "outputs": [],
   "source": [
    "n_hidden = 8  # number of hidden units\n",
    "\n",
    "model_fnn = FullyConnected2Layers(input_size, n_hidden, output_size)\n",
    "model_fnn.to(device)\n",
    "optimizer = optim.SGD(model_fnn.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "print(\"Number of parameters: {}\".format(get_n_params(model_fnn)))\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    train(epoch, model_fnn)\n",
    "    test(model_fnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb3a3b0",
   "metadata": {
    "id": "4d9fe8fc"
   },
   "source": [
    "A CNN with the same number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ebf1df",
   "metadata": {
    "id": "01a9716d"
   },
   "outputs": [],
   "source": [
    "# Training settings\n",
    "n_features = 6  # number of feature maps\n",
    "\n",
    "model_cnn = CNN(input_size, n_features, output_size)\n",
    "model_cnn.to(device)\n",
    "optimizer = optim.SGD(model_cnn.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "print(\"Number of parameters: {}\".format(get_n_params(model_cnn)))\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    train(epoch, model_cnn)\n",
    "    test(model_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596cda6",
   "metadata": {
    "id": "b6dd397f"
   },
   "source": [
    "The ConvNet performs better with the same number of parameters, thanks to its use of prior knowledge about images\n",
    "\n",
    "    Use of convolution: Locality and stationarity in images\n",
    "    Pooling: builds in some translation invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4f7d27",
   "metadata": {
    "id": "51c93c84"
   },
   "source": [
    "### What happens when CNNs assumptions are not true ?\n",
    "\n",
    "We will deterministically permute pixels so that the content of an image is respected but not its structure\n",
    "\n",
    "Basically transform some positions into others\n",
    "\n",
    "And we will train networks on this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae593936",
   "metadata": {
    "id": "fa5d3bc5"
   },
   "outputs": [],
   "source": [
    "perm = torch.randperm(784)\n",
    "plt.figure(figsize=(16, 12))\n",
    "for i in range(10):\n",
    "    image, _ = train_loader.dataset.__getitem__(i)\n",
    "    # permute pixels\n",
    "    image_perm = image.view(-1, 28 * 28).clone()\n",
    "    image_perm = image_perm[:, perm]\n",
    "    image_perm = image_perm.view(-1, 1, 28, 28)\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    plt.imshow(image.squeeze().numpy())\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(4, 5, i + 11)\n",
    "    plt.imshow(image_perm.squeeze().numpy())\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e130b7",
   "metadata": {
    "id": "37de1337"
   },
   "outputs": [],
   "source": [
    "# Training settings\n",
    "n_features = 6  # number of feature maps\n",
    "\n",
    "model_cnn = CNN(input_size, n_features, output_size)\n",
    "model_cnn.to(device)\n",
    "optimizer = optim.SGD(model_cnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print(\"Number of parameters: {}\".format(get_n_params(model_cnn)))\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    train(epoch, model_cnn, perm)\n",
    "    test(model_cnn, perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a07616",
   "metadata": {
    "id": "6441b819"
   },
   "outputs": [],
   "source": [
    "n_hidden = 8  # number of hidden units\n",
    "\n",
    "model_fnn = FullyConnected2Layers(input_size, n_hidden, output_size)\n",
    "model_fnn.to(device)\n",
    "optimizer = optim.SGD(model_fnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print(\"Number of parameters: {}\".format(get_n_params(model_fnn)))\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    train(epoch, model_fnn, perm)\n",
    "    test(model_fnn, perm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67e1690",
   "metadata": {
    "id": "11d0fb79"
   },
   "source": [
    "**Takeaway messages**\n",
    "\n",
    "The ConvNet's performance drops when we permute the pixels, but the Fully-Connected Network's performance stays the same\n",
    "\n",
    "    ConvNet makes the assumption that pixels lie on a grid and are stationary/local\n",
    "    It loses performance when this assumption is wrong\n",
    "    The fully-connected network does not make this assumption\n",
    "    It does less well when it is true, since it doesn't take advantage of this prior knowledge\n",
    "    But it doesn't suffer when the assumption is wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98257ed",
   "metadata": {
    "id": "8ce44a07"
   },
   "outputs": [],
   "source": [
    "plt.bar(\n",
    "    (\"NN normal\", \"CNN normal\", \"CNN scrambled\", \"NN scrambled\"),\n",
    "    accuracy_list,\n",
    "    width=0.4,\n",
    ")\n",
    "plt.ylim((min(accuracy_list) - 5, 96))\n",
    "plt.ylabel(\"Accuracy [%]\")\n",
    "for tick in plt.gca().xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(10)\n",
    "plt.title(\"Performance comparison\");"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
